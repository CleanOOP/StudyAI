{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CleanOOP/StudyAI/blob/Week1Homework/1_1_Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression 실습\n",
        "\n",
        "이번 실습에서는 linear regression에 대한 gradient descent를 직접 구현해봅니다. 여기서 사용할 문제들은 크게 두 가지로 OR 문제와 XOR 문제입니다.\n",
        "\n",
        "먼저 필요한 library들을 import합시다."
      ],
      "metadata": {
        "id": "dS_t0-ik_WC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DEJFJkL6qHB9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OR Problem\n",
        "\n",
        "OR은 0 또는 1의 값을 가질 수 있는 두 개의 정수를 입력으로 받아 둘 중에 하나라도 1이면 1을 출력하고 아니면 0을 출력하는 문제입니다.\n",
        "즉, 우리가 학습하고자 하는 함수는 2개의 정수를 입력받아 하나의 정수를 출력하면됩니다. 이러한 함수를 학습하기 위한 data는 다음과 같이 구성할 수 있습니다."
      ],
      "metadata": {
        "id": "cG2fJsOF8LsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "y = torch.tensor([0, 1, 1, 1])\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "SsEdD6T7qLJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c4dc7a-638a-4d07-8529-fc0c35f4208e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 결과에서 볼 수 있다시피 $x$의 shape은 (4, 2)로, 총 4개의 two-dimensional data 임을 알 수 있습니다. $y$는 각 $x_i$에 대한 label로 우리가 설정한 문제의 조건을 잘 따라가는 것을 알 수 있습니다.\n",
        "\n",
        "다음으로는 linear regression의 parameter들인 $w, b$를 정의하겠습니다."
      ],
      "metadata": {
        "id": "YyD1n6wf_3ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn((1, 2))\n",
        "b = torch.randn((1, 1))\n",
        "\n",
        "print(w.shape, b.shape)"
      ],
      "metadata": {
        "id": "uzG4w1VYqlhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fecf28-5ab0-4ed0-f617-3523efe1ccda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2]) torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$w$는 1x2의 벡터이고 $b$는 1x1의 scalar임을 알 수 있습니다. 여기서는 `torch.randn`을 사용하여 standard normal distribution을 가지고 초기화하였습니다.\n",
        "\n",
        "이러한 $w, b$와 data $x, y$가 주어졌을 때 우리가 학습한 $w, b$의 성능을 평가하는 함수를 구현합시다.\n",
        "평가 함수는 다음과 같이 MSE로 정의됩니다:\n",
        "$$l(f) := MSE(f(x), y) = \\frac{1}{n} \\sum_{i=1}^n (f(x_i) - y)^2.$$\n",
        "이를 구현한 코드는 다음과 같습니다."
      ],
      "metadata": {
        "id": "ELTb9Dl-AYbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(w, b, x):\n",
        "  return torch.matmul(w, x.T) + b\n",
        "\n",
        "\n",
        "def loss(w, b, x, y):\n",
        "  return (y - pred(w, b, x)).pow(2).mean()"
      ],
      "metadata": {
        "id": "LBxldV7D8UMf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 `def pred(w, b, x)`는 $wx^T + b$, 즉 1차 함수 $f$의 $x$에 대한 결과를 반환하는 함수를 구현했습니다.\n",
        "이를 이용하여 주어진 label $y$와의 MSE를 측정하는 코드가 `def loss(w, b, x, y)`에 구현되어있습니다.\n",
        "\n",
        "다음은 MSE를 기반으로 $w, b$의 gradient를 구하는 코드를 구현하겠습니다.\n",
        "MSE에 대한 $w$의 gradient는 다음과 같이 구할 수 있습니다:\n",
        "$$\\frac{\\partial l}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y)x_i.$$\n",
        "$b$에 대한 gradient는 다음과 같습니다:\n",
        "$$\\frac{\\partial l}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y).$$\n",
        "이를 코드로 구현하면 다음과 같습니다."
      ],
      "metadata": {
        "id": "gmM79Ly6VyBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_w(w, b, x, y):\n",
        "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
        "  tmp1 = torch.matmul(w, x.T)  # (1, n)\n",
        "  tmp2 = tmp1 + b              # (1, n)\n",
        "  tmp3 = 2 * (tmp2 - y[None])  # (1, n)\n",
        "  grad_item = tmp3.T * x       # (n, d)\n",
        "  return grad_item.mean(dim=0, keepdim=True)  # (1, d)\n",
        "\n",
        "\n",
        "def grad_b(w, b, x, y):\n",
        "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
        "  grad_item = 2 * (torch.matmul(w, x.T) + b - y[None])  # (1, n)\n",
        "  return grad_item.mean(dim=-1, keepdim=True)           # (1, 1)"
      ],
      "metadata": {
        "id": "rLrsXZ0iq13m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 중요한 것은 shape에 맞춰서 연산을 잘 사용해야한다는 것입니다. Shape과 관련된 설명은 `[Chapter 0]`의 Numpy에서 설명했으니, 복습하신다는 느낌으로 주석으로 써놓은 shape들을 유도해보시면 좋을 것 같습니다. 중요한 것은 반환되는 tensor의 shape이 우리가 구하고자 하는 gradient와 일치해야 한다는 것입니다. 예를 들어 $w$의 $l$에 대한 gradient는 $w$와 shape이 동일해야 합니다.\n",
        "\n",
        "마지막으로 gradient descent 함수를 구현하겠습니다. Gradient descent는 다음과 같이 정의됩니다:\n",
        "$$w^{(new)} = w^{(old)} - \\eta \\frac{\\partial l}{\\partial w} \\biggr\\rvert_{w = w^{(old)}}.$$\n",
        "Gradient는 위에서 구현했으니 이를 활용하여 learning rate $\\eta$가 주어졌을 때 $w, b$를 update하는 코드를 구현할 수 있습니다. 구현한 결과는 다음과 같습니다."
      ],
      "metadata": {
        "id": "mCbBU1RaX6O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(x, y, w, b, lr):\n",
        "  w = w - lr * grad_w(w, b, x, y)\n",
        "  b = b - lr * grad_b(w, b, x, y)\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "wFRS72UF8QVv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent에 해당하는 코드는 모두 구현하였습니다. 이제 학습하는 코드를 구현하겠습니다:"
      ],
      "metadata": {
        "id": "b93uvneVZ7bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs, lr, w, b, x, y):\n",
        "  for e in range(n_epochs):\n",
        "    w, b = update(x, y, w, b, lr)\n",
        "    print(f\"Epoch {e:3d} | Loss: {loss(w, b, x, y)}\")\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "Pa6fA_ZUFI-0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 `n_epochs`는 update를 하는 횟수를 의미합니다. 매 update 이후에 `loss` 함수를 사용하여 잘 수렴하고 있는지 살펴봅니다. 실제로 이 함수를 실행한 결과는 다음과 같습니다."
      ],
      "metadata": {
        "id": "GrJGKWilaBFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "lr = 0.1\n",
        "\n",
        "w, b = train(n_epochs, lr, w, b, x, y)\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFk-josgBSj7",
        "outputId": "cec66f87-a581-47f8-e6c0-5f2bdd022916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Loss: 0.19179020822048187\n",
            "Epoch   1 | Loss: 0.17894583940505981\n",
            "Epoch   2 | Loss: 0.16748864948749542\n",
            "Epoch   3 | Loss: 0.15721547603607178\n",
            "Epoch   4 | Loss: 0.14797675609588623\n",
            "Epoch   5 | Loss: 0.13965463638305664\n",
            "Epoch   6 | Loss: 0.13215114176273346\n",
            "Epoch   7 | Loss: 0.125382199883461\n",
            "Epoch   8 | Loss: 0.11927397549152374\n",
            "Epoch   9 | Loss: 0.11376101523637772\n",
            "Epoch  10 | Loss: 0.1087847501039505\n",
            "Epoch  11 | Loss: 0.10429259389638901\n",
            "Epoch  12 | Loss: 0.10023726522922516\n",
            "Epoch  13 | Loss: 0.09657621383666992\n",
            "Epoch  14 | Loss: 0.0932709351181984\n",
            "Epoch  15 | Loss: 0.09028685837984085\n",
            "Epoch  16 | Loss: 0.08759266883134842\n",
            "Epoch  17 | Loss: 0.08516019582748413\n",
            "Epoch  18 | Loss: 0.08296399563550949\n",
            "Epoch  19 | Loss: 0.08098104596138\n",
            "Epoch  20 | Loss: 0.07919059693813324\n",
            "Epoch  21 | Loss: 0.07757396996021271\n",
            "Epoch  22 | Loss: 0.07611428201198578\n",
            "Epoch  23 | Loss: 0.07479622960090637\n",
            "Epoch  24 | Loss: 0.07360605895519257\n",
            "Epoch  25 | Loss: 0.07253137230873108\n",
            "Epoch  26 | Loss: 0.07156091183423996\n",
            "Epoch  27 | Loss: 0.07068457454442978\n",
            "Epoch  28 | Loss: 0.06989321112632751\n",
            "Epoch  29 | Loss: 0.06917857378721237\n",
            "Epoch  30 | Loss: 0.06853318959474564\n",
            "Epoch  31 | Loss: 0.06795034557580948\n",
            "Epoch  32 | Loss: 0.0674239844083786\n",
            "Epoch  33 | Loss: 0.06694860011339188\n",
            "Epoch  34 | Loss: 0.06651924550533295\n",
            "Epoch  35 | Loss: 0.06613147258758545\n",
            "Epoch  36 | Loss: 0.06578122824430466\n",
            "Epoch  37 | Loss: 0.06546488404273987\n",
            "Epoch  38 | Loss: 0.06517914682626724\n",
            "Epoch  39 | Loss: 0.06492104381322861\n",
            "Epoch  40 | Loss: 0.06468790769577026\n",
            "Epoch  41 | Loss: 0.06447730958461761\n",
            "Epoch  42 | Loss: 0.06428704410791397\n",
            "Epoch  43 | Loss: 0.06411518901586533\n",
            "Epoch  44 | Loss: 0.06395989656448364\n",
            "Epoch  45 | Loss: 0.06381963193416595\n",
            "Epoch  46 | Loss: 0.06369288265705109\n",
            "Epoch  47 | Loss: 0.06357838213443756\n",
            "Epoch  48 | Loss: 0.06347492337226868\n",
            "Epoch  49 | Loss: 0.06338143348693848\n",
            "Epoch  50 | Loss: 0.06329695135354996\n",
            "Epoch  51 | Loss: 0.06322062015533447\n",
            "Epoch  52 | Loss: 0.06315164268016815\n",
            "Epoch  53 | Loss: 0.0630892887711525\n",
            "Epoch  54 | Loss: 0.06303295493125916\n",
            "Epoch  55 | Loss: 0.06298201531171799\n",
            "Epoch  56 | Loss: 0.06293600052595139\n",
            "Epoch  57 | Loss: 0.06289440393447876\n",
            "Epoch  58 | Loss: 0.0628567710518837\n",
            "Epoch  59 | Loss: 0.06282278150320053\n",
            "Epoch  60 | Loss: 0.06279204040765762\n",
            "Epoch  61 | Loss: 0.06276427954435349\n",
            "Epoch  62 | Loss: 0.06273914128541946\n",
            "Epoch  63 | Loss: 0.06271642446517944\n",
            "Epoch  64 | Loss: 0.06269587576389313\n",
            "Epoch  65 | Loss: 0.062677301466465\n",
            "Epoch  66 | Loss: 0.06266050785779953\n",
            "Epoch  67 | Loss: 0.062645323574543\n",
            "Epoch  68 | Loss: 0.06263156235218048\n",
            "Epoch  69 | Loss: 0.06261913478374481\n",
            "Epoch  70 | Loss: 0.06260789185762405\n",
            "Epoch  71 | Loss: 0.06259772926568985\n",
            "Epoch  72 | Loss: 0.06258849799633026\n",
            "Epoch  73 | Loss: 0.06258019804954529\n",
            "Epoch  74 | Loss: 0.06257263571023941\n",
            "Epoch  75 | Loss: 0.06256582587957382\n",
            "Epoch  76 | Loss: 0.06255966424942017\n",
            "Epoch  77 | Loss: 0.06255406886339188\n",
            "Epoch  78 | Loss: 0.06254900991916656\n",
            "Epoch  79 | Loss: 0.06254442036151886\n",
            "Epoch  80 | Loss: 0.06254027783870697\n",
            "Epoch  81 | Loss: 0.06253653764724731\n",
            "Epoch  82 | Loss: 0.06253314018249512\n",
            "Epoch  83 | Loss: 0.06253005564212799\n",
            "Epoch  84 | Loss: 0.06252726912498474\n",
            "Epoch  85 | Loss: 0.06252475082874298\n",
            "Epoch  86 | Loss: 0.06252244859933853\n",
            "Epoch  87 | Loss: 0.06252037733793259\n",
            "Epoch  88 | Loss: 0.06251850724220276\n",
            "Epoch  89 | Loss: 0.06251679360866547\n",
            "Epoch  90 | Loss: 0.06251523643732071\n",
            "Epoch  91 | Loss: 0.06251385807991028\n",
            "Epoch  92 | Loss: 0.06251257658004761\n",
            "Epoch  93 | Loss: 0.06251144409179688\n",
            "Epoch  94 | Loss: 0.06251037865877151\n",
            "Epoch  95 | Loss: 0.0625094324350357\n",
            "Epoch  96 | Loss: 0.06250859051942825\n",
            "Epoch  97 | Loss: 0.06250780820846558\n",
            "Epoch  98 | Loss: 0.06250710040330887\n",
            "Epoch  99 | Loss: 0.06250645965337753\n",
            "tensor([[0.4986, 0.5048]]) tensor([[0.2480]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "잘 수렴하는 것을 확인하였습니다. 마지막으로 OR data에 대한 $w, b$의 예측 결과와 label을 비교해봅시다."
      ],
      "metadata": {
        "id": "y2Ny-YkAaNh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred(w, b, x))\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggGP969Bh-w",
        "outputId": "dd978da3-41ae-4a3f-d569-28b612909a97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2480, 0.7529, 0.7466, 1.2514]])\n",
            "tensor([0, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측 결과를 볼 수 있다시피 우리의 linear regression model은 0과 1에 해당하는 data를 잘 구분하는 것을 알 수 있습니다."
      ],
      "metadata": {
        "id": "F8gKvx2naWDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Problem\n",
        "\n",
        "이번에는 XOR를 학습해보겠습니다. XOR은 OR과 똑같은 입력을 받는 문제로, 두 개의 0 또는 1의 정수가 들어왔을 때 두 정수가 다르면 1, 아니면 0을 출력해야 합니다.\n",
        "먼저 data를 만들어보겠습니다:"
      ],
      "metadata": {
        "id": "zMXZLfd3DC50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "y = torch.tensor([0, 1, 1, 0])\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtFGsqNXCjtM",
        "outputId": "89da07a1-1c90-4d1c-8bae-f826095f9a3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "보시다시피 shape이나 생성 과정은 OR과 똑같습니다. 다른 것은 $y$에서의 labeling입니다. OR과 다르게 $x = (1, 1)$에 대해서는 0을 labeling했습니다.\n",
        "이러한 사소한 차이에 대해서도 linear regression model이 잘 학습할 수 있을지 살펴보겠습니다."
      ],
      "metadata": {
        "id": "iYRtKaviaedO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "lr = 0.1\n",
        "\n",
        "w, b = train(n_epochs, lr, w, b, x, y)\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw5UUqKdDG98",
        "outputId": "62b4a8ca-a807-49ad-bdaa-f3cb976b08e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Loss: 0.3775087893009186\n",
            "Epoch   1 | Loss: 0.3447214365005493\n",
            "Epoch   2 | Loss: 0.32608547806739807\n",
            "Epoch   3 | Loss: 0.3147139549255371\n",
            "Epoch   4 | Loss: 0.30714818835258484\n",
            "Epoch   5 | Loss: 0.3016403913497925\n",
            "Epoch   6 | Loss: 0.29729896783828735\n",
            "Epoch   7 | Loss: 0.2936630845069885\n",
            "Epoch   8 | Loss: 0.2904892563819885\n",
            "Epoch   9 | Loss: 0.2876449227333069\n",
            "Epoch  10 | Loss: 0.2850548028945923\n",
            "Epoch  11 | Loss: 0.2826734185218811\n",
            "Epoch  12 | Loss: 0.2804712653160095\n",
            "Epoch  13 | Loss: 0.2784278690814972\n",
            "Epoch  14 | Loss: 0.27652767300605774\n",
            "Epoch  15 | Loss: 0.27475830912590027\n",
            "Epoch  16 | Loss: 0.27310943603515625\n",
            "Epoch  17 | Loss: 0.2715718448162079\n",
            "Epoch  18 | Loss: 0.27013757824897766\n",
            "Epoch  19 | Loss: 0.2687993347644806\n",
            "Epoch  20 | Loss: 0.26755040884017944\n",
            "Epoch  21 | Loss: 0.2663847804069519\n",
            "Epoch  22 | Loss: 0.2652967870235443\n",
            "Epoch  23 | Loss: 0.26428115367889404\n",
            "Epoch  24 | Loss: 0.2633330225944519\n",
            "Epoch  25 | Loss: 0.26244792342185974\n",
            "Epoch  26 | Loss: 0.2616215944290161\n",
            "Epoch  27 | Loss: 0.2608501613140106\n",
            "Epoch  28 | Loss: 0.2601299285888672\n",
            "Epoch  29 | Loss: 0.259457528591156\n",
            "Epoch  30 | Loss: 0.258829802274704\n",
            "Epoch  31 | Loss: 0.25824373960494995\n",
            "Epoch  32 | Loss: 0.25769656896591187\n",
            "Epoch  33 | Loss: 0.257185697555542\n",
            "Epoch  34 | Loss: 0.2567087411880493\n",
            "Epoch  35 | Loss: 0.25626349449157715\n",
            "Epoch  36 | Loss: 0.2558477520942688\n",
            "Epoch  37 | Loss: 0.25545960664749146\n",
            "Epoch  38 | Loss: 0.25509727001190186\n",
            "Epoch  39 | Loss: 0.25475892424583435\n",
            "Epoch  40 | Loss: 0.25444304943084717\n",
            "Epoch  41 | Loss: 0.2541481852531433\n",
            "Epoch  42 | Loss: 0.2538728415966034\n",
            "Epoch  43 | Loss: 0.2536157965660095\n",
            "Epoch  44 | Loss: 0.2533757984638214\n",
            "Epoch  45 | Loss: 0.2531517446041107\n",
            "Epoch  46 | Loss: 0.2529425323009491\n",
            "Epoch  47 | Loss: 0.25274723768234253\n",
            "Epoch  48 | Loss: 0.2525649070739746\n",
            "Epoch  49 | Loss: 0.2523946762084961\n",
            "Epoch  50 | Loss: 0.2522357106208801\n",
            "Epoch  51 | Loss: 0.2520873248577118\n",
            "Epoch  52 | Loss: 0.2519487738609314\n",
            "Epoch  53 | Loss: 0.2518194615840912\n",
            "Epoch  54 | Loss: 0.25169867277145386\n",
            "Epoch  55 | Loss: 0.2515859007835388\n",
            "Epoch  56 | Loss: 0.25148066878318787\n",
            "Epoch  57 | Loss: 0.25138238072395325\n",
            "Epoch  58 | Loss: 0.25129061937332153\n",
            "Epoch  59 | Loss: 0.2512049674987793\n",
            "Epoch  60 | Loss: 0.2511250078678131\n",
            "Epoch  61 | Loss: 0.25105032324790955\n",
            "Epoch  62 | Loss: 0.2509806156158447\n",
            "Epoch  63 | Loss: 0.25091552734375\n",
            "Epoch  64 | Loss: 0.2508547306060791\n",
            "Epoch  65 | Loss: 0.2507980465888977\n",
            "Epoch  66 | Loss: 0.2507450580596924\n",
            "Epoch  67 | Loss: 0.2506955862045288\n",
            "Epoch  68 | Loss: 0.25064945220947266\n",
            "Epoch  69 | Loss: 0.25060635805130005\n",
            "Epoch  70 | Loss: 0.25056612491607666\n",
            "Epoch  71 | Loss: 0.2505285143852234\n",
            "Epoch  72 | Loss: 0.25049343705177307\n",
            "Epoch  73 | Loss: 0.250460684299469\n",
            "Epoch  74 | Loss: 0.2504301071166992\n",
            "Epoch  75 | Loss: 0.2504015564918518\n",
            "Epoch  76 | Loss: 0.2503749132156372\n",
            "Epoch  77 | Loss: 0.2503500282764435\n",
            "Epoch  78 | Loss: 0.2503267824649811\n",
            "Epoch  79 | Loss: 0.2503051161766052\n",
            "Epoch  80 | Loss: 0.2502848505973816\n",
            "Epoch  81 | Loss: 0.2502659559249878\n",
            "Epoch  82 | Loss: 0.2502482831478119\n",
            "Epoch  83 | Loss: 0.2502318024635315\n",
            "Epoch  84 | Loss: 0.25021642446517944\n",
            "Epoch  85 | Loss: 0.2502020597457886\n",
            "Epoch  86 | Loss: 0.2501886487007141\n",
            "Epoch  87 | Loss: 0.2501761317253113\n",
            "Epoch  88 | Loss: 0.2501644492149353\n",
            "Epoch  89 | Loss: 0.250153511762619\n",
            "Epoch  90 | Loss: 0.2501433491706848\n",
            "Epoch  91 | Loss: 0.25013381242752075\n",
            "Epoch  92 | Loss: 0.2501249313354492\n",
            "Epoch  93 | Loss: 0.25011664628982544\n",
            "Epoch  94 | Loss: 0.25010889768600464\n",
            "Epoch  95 | Loss: 0.2501017153263092\n",
            "Epoch  96 | Loss: 0.2500949203968048\n",
            "Epoch  97 | Loss: 0.2500886023044586\n",
            "Epoch  98 | Loss: 0.25008273124694824\n",
            "Epoch  99 | Loss: 0.2500772774219513\n",
            "tensor([[0.0121, 0.0121]]) tensor([[0.4859]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전과는 다르게 loss가 1.0보다 작아지지 않는 것을 알 수 있습니다. 실제 예측 결과를 살펴보면 다음과 같습니다."
      ],
      "metadata": {
        "id": "i8sMLaJ9a770"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred(w, b, x))\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L81iXxgHDIq2",
        "outputId": "734bcafb-2079-4a36-efd4-9a207ff943d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4859, 0.4981, 0.4980, 0.5101]])\n",
            "tensor([0, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "보시다시피 0과 1에 해당하는 data들을 잘 구분하지 못하는 모습니다. Linear regression model은 XOR을 잘 처리하지 못하는 것을 우리는 이번 실습을 통해 알 수 있습니다."
      ],
      "metadata": {
        "id": "MuqkwJ2NbB7S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zAy7YgFDMgx"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}